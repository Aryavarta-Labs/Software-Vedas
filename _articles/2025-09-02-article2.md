---
title: "âš¡ Low-Level Design of Caching in Node.js"
author: "Akash Srivastava"
date: 2025-09-01
summary: "A deep dive into TTL and LRU caching strategies with low-level design (LLD) and Node.js implementations."
---

# âš¡ Low-Level Design of Caching in Node.js

## Introduction

Caching is a fundamental optimization technique that improves system performance and reduces latency by storing copies of frequently accessed data closer to the computation. Whether you're designing a microservice, a web server, or a distributed system, implementing a robust caching strategy is critical.

In this article, weâ€™ll dive into the low-level design (LLD) of caching systems using **Node.js**, focusing on two popular strategies:

- **TTL (Time-To-Live) based caching**
- **LRU (Least Recently Used) caching**

We'll explore concepts, use cases, and build a lightweight implementation of both using Node.js.

---

## Why Caching?

Letâ€™s say your app frequently fetches product details from a database. The DB access might take **100ms per query**. If you cache this data, subsequent requests can be served in **2ms from memory** â€” a **50x performance gain**.

Other reasons to use caching:

- Reduce database load
- Improve API response times
- Save bandwidth
- Increase throughput

---

## Low-Level Design of Caching

Letâ€™s break down a generic low-level caching module:

### Core Requirements

- `get(key)`: Fetch value from cache  
- `set(key, value, options?)`: Add value to cache

### Optional Features

- Auto-expiry via TTL  
- Memory limit eviction using LRU  
- Hit/miss counters

---

## TTL (Time-To-Live) Caching

TTL sets a fixed lifespan on a cached item. Once it expires, the item is purged or ignored.

### Use Cases

- API rate limits  
- Temporary authentication tokens  
- Real-time market data  
- Weather or location services

### TTL Design Components

Each cache entry stores: `{Value & Expiry timestamp}`  
A cleanup process runs periodically or on `get()` call

### Reference Code (TDD)

#### Development Code
```js
class TTLCache {
  constructor() {
    this.store = new Map();
  }

  set(key, value, ttlMs) {
    const expiry = Date.now() + ttlMs;
    this.store.set(key, { value, expiry });
  }

  get(key) {
    const data = this.store.get(key);
    if (!data) return null;

    if (Date.now() > data.expiry) {
      this.store.delete(key);
      return null; // expired
    }
    return data.value;
  }

  delete(key) {
    return this.store.delete(key);
  }

  size() {
    return this.store.size;
  }
}
```

#### Testing Code
```js
const cache = new TTLCache();
cache.set('user:123', { name: 'Alice' }, 3000); // 3 sec TTL

setTimeout(() => {
  console.log(cache.get('user:123')); // null after 3 sec
}, 3500);
```

---

## LRU (Least Recently Used) Caching

LRU discards the least recently accessed items when the cache exceeds its capacity.

### Use Cases

- Image thumbnail caching  
- Browser caches  
- In-memory DB query results

### LRU Design Components

You need:

- A hashmap for O(1) access to cache values
- A doubly linked list to maintain usage order

Most recently accessed â†’ front  
Least recently accessed â†’ back  
On cache overflow â†’ remove from back

### Reference Code (TDD)

#### Development Code
```js
class Node {
  constructor(key, value) {
    this.key = key;
    this.value = value;
    this.prev = this.next = null;
  }
}

class LRUCache {
  constructor(limit = 5) {
    this.limit = limit;
    this.map = new Map();
    this.head = this.tail = null;
  }

  _remove(node) {
    if (node.prev) node.prev.next = node.next;
    if (node.next) node.next.prev = node.prev;
    if (node === this.head) this.head = node.next;
    if (node === this.tail) this.tail = node.prev;
    node.prev = node.next = null;
  }

  _addToFront(node) {
    node.next = this.head;
    if (this.head) this.head.prev = node;
    this.head = node;
    if (!this.tail) this.tail = node;
  }

  get(key) {
    if (!this.map.has(key)) return null;
    const node = this.map.get(key);
    this._remove(node);
    this._addToFront(node);
    return node.value;
  }

  set(key, value) {
    if (this.map.has(key)) {
      const existing = this.map.get(key);
      this._remove(existing);
    } else if (this.map.size >= this.limit) {
      this.map.delete(this.tail.key);
      this._remove(this.tail);
    }

    const node = new Node(key, value);
    this._addToFront(node);
    this.map.set(key, node);
  }

  keys() {
    return Array.from(this.map.keys());
  }
}
```

#### Testing Code
```js
const lru = new LRUCache(3);
lru.set('a', 1);
lru.set('b', 2);
lru.set('c', 3);
lru.get('a'); // a becomes most recently used
lru.set('d', 4); // 'b' is evicted (least recently used)

console.log(lru.keys()); // ['a', 'c', 'd']
```

---

## Hybrid Caching: LRU + TTL

In real-world systems (e.g., Redis, Memcached), **TTL and LRU are often combined**.

- Each item has a TTL
- Cache evicts based on LRU when memory exceeds threshold
- Items can also expire by time

This provides both **freshness** and **memory efficiency**.

ðŸ‘‰ You can extend the above Node.js examples to support both TTL and LRU by adding a timestamp and checking expiry before reordering the nodes.

---

## Conclusion

Caching is both an **art and science**. With thoughtful low-level design:

- **TTL** keeps data fresh and temporary
- **LRU** ensures memory is used efficiently

Node.js makes it easy to build and prototype caching logic for small to medium-scale apps. For production use, prefer proven libraries like **node-cache**, **lru-cache**, or **Redis clients**.

---

