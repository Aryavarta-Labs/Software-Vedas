---
title: "⚡ Low-Level Design of Caching in Node.js"
author: "Akash Srivastava"
date: 2025-09-01
summary: "A deep dive into TTL and LRU caching strategies with low-level design (LLD) and Node.js implementations."
---

# ⚡ Low-Level Design of Caching in Node.js

## 🚀 Introduction

Caching is one of the simplest yet most powerful optimizations in system design.  
It **reduces latency** and **improves performance** by storing frequently accessed data closer to the application.

Whether you’re building:

- a **microservice**,  
- a **web server**, or  
- a **distributed system**,  

…a **robust caching strategy** is critical.

In this guide, we’ll explore the **Low-Level Design (LLD)** of caching in **Node.js**, focusing on two widely used strategies:

- 🕒 **TTL (Time-To-Live) Caching**  
- 🔄 **LRU (Least Recently Used) Caching**

We’ll cover **concepts, use cases, and code implementations** step by step.

---

## ❓ Why Caching?

Imagine your app frequently queries product details from a database.  
Each DB query costs **~100ms**, but fetching from memory cache costs **~2ms**.

That’s a **50x speedup** ⚡.

Other benefits:

- Reduce database load  
- Improve API response times  
- Save bandwidth  
- Increase throughput  

---

## 🧩 Low-Level Design of a Cache

At the core, a cache system provides two operations:

- `get(key)` → fetch value from cache  
- `set(key, value, options?)` → insert value with optional rules (like TTL)  

### Optional Features

- **TTL (expiry)** → data auto-expires  
- **LRU (eviction)** → memory limit enforcement  
- **Stats** → hit/miss counters  

---

## 🕒 TTL (Time-To-Live) Caching

TTL means: **“store this item, but only for X ms.”**  
After expiry, it’s **purged** or **ignored**.

### ✅ Use Cases
- API rate limits  
- Auth tokens  
- Weather data  
- Real-time market feeds  

### 🏗️ Design Idea
Each cache entry = `{ value, expiryTimestamp }`  

A **cleanup check** runs:
- during `get()` calls, or  
- periodically in the background.  

### 📦 Implementation

```js
class TTLCache {
  constructor() {
    this.store = new Map();
  }

  set(key, value, ttlMs) {
    const expiry = Date.now() + ttlMs;
    this.store.set(key, { value, expiry });
  }

  get(key) {
    const data = this.store.get(key);
    if (!data) return null;

    if (Date.now() > data.expiry) {
      this.store.delete(key); // expired
      return null;
    }
    return data.value;
  }

  delete(key) {
    return this.store.delete(key);
  }

  size() {
    return this.store.size;
  }
}
```

### 🧪 Quick Test
```js
const cache = new TTLCache();
cache.set('user:123', { name: 'Alice' }, 3000);

setTimeout(() => {
  console.log(cache.get('user:123')); // null (expired after 3s)
}, 3500);
```

---

## 🔄 LRU (Least Recently Used) Caching

LRU means: **when memory is full, remove the least recently used item first**.

### ✅ Use Cases
- Browser caches  
- Image thumbnails  
- In-memory DB query results  

### 🏗️ Design Idea
- **HashMap** → O(1) access to items  
- **Doubly Linked List** → track usage order  
  - Head = most recently used  
  - Tail = least recently used (evict this first)  

### 📦 Implementation

```js
class Node {
  constructor(key, value) {
    this.key = key;
    this.value = value;
    this.prev = this.next = null;
  }
}

class LRUCache {
  constructor(limit = 5) {
    this.limit = limit;
    this.map = new Map();
    this.head = this.tail = null;
  }

  _remove(node) {
    if (node.prev) node.prev.next = node.next;
    if (node.next) node.next.prev = node.prev;
    if (node === this.head) this.head = node.next;
    if (node === this.tail) this.tail = node.prev;
    node.prev = node.next = null;
  }

  _addToFront(node) {
    node.next = this.head;
    if (this.head) this.head.prev = node;
    this.head = node;
    if (!this.tail) this.tail = node;
  }

  get(key) {
    if (!this.map.has(key)) return null;
    const node = this.map.get(key);
    this._remove(node);
    this._addToFront(node);
    return node.value;
  }

  set(key, value) {
    if (this.map.has(key)) {
      this._remove(this.map.get(key));
    } else if (this.map.size >= this.limit) {
      this.map.delete(this.tail.key);
      this._remove(this.tail);
    }

    const node = new Node(key, value);
    this._addToFront(node);
    this.map.set(key, node);
  }

  keys() {
    return Array.from(this.map.keys());
  }
}
```

### 🧪 Quick Test
```js
const lru = new LRUCache(3);
lru.set('a', 1);
lru.set('b', 2);
lru.set('c', 3);
lru.get('a');     // 'a' = most recently used
lru.set('d', 4);  // 'b' gets evicted

console.log(lru.keys()); // ['a', 'c', 'd']
```

---

## 🔀 Hybrid Caching: LRU + TTL

Real-world caches (Redis, Memcached) **combine LRU and TTL**:

- Each entry has **TTL** (ensures freshness)  
- Memory cap uses **LRU eviction**  
- Items may expire **either by time or memory pressure**  

👉 Extend the above LRU implementation by adding an **expiry timestamp** to each node, and check expiry before moving nodes.

---

## 🎯 Conclusion

Caching is both **art and engineering**.  

- **TTL** → guarantees freshness (time-based expiry)  
- **LRU** → ensures efficient memory use (usage-based eviction)  

Node.js makes prototyping such strategies simple.  
For production, use **battle-tested libraries** like:  
- [node-cache](https://www.npmjs.com/package/node-cache)  
- [lru-cache](https://www.npmjs.com/package/lru-cache)  
- or external stores like **Redis**  

---

✅ Now you’ve got a **clear low-level design** and **working Node.js code** for building your own cache!
